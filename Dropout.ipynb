{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The organization of this class definition is inspired by some OOP-flavored code I found at\n",
    "#https://thenewphalls.wordpress.com/2014/02/19/understanding-object-oriented-programming-in-julia-part-1/\n",
    "\n",
    "type NeuralNet     #create by e.g. \"net = NeuralNet([42 100 100 1])\"\n",
    "                   # would one output node, 42 input nodes, and two hidden layers of 100 nodes each\n",
    "    \n",
    "    layers;        #an integer containing the number of hidden layers + 1\n",
    "    nodes;         #an array containing the size of the net as described above\n",
    "    weights;       #an array of matrices containing the weights from one layer of the net to the next\n",
    "    biases;        #an array of vectors containing the bias/threshold that influences a node's activation\n",
    "    \n",
    "    function NeuralNet(nodes)               #constructor\n",
    "        \n",
    "        this = new();                       #create object\n",
    "        this.nodes = nodes;                 #store node size information in the object\n",
    "        \n",
    "        this.layers = size(nodes)[2] - 1;   #figure out how many layers from the constructor's input\n",
    "        \n",
    "        this.weights = Array[];             #initialize\n",
    "        this.biases = Array[];\n",
    "        \n",
    "        for i = 1:this.layers\n",
    "            \n",
    "            push!(this.weights, 0.1*rand(nodes[i+1], nodes[i]));\n",
    "                                            #create weight matrices\n",
    "                                            #this will result in an array of matrices\n",
    "            push!(this.biases, 0.01*rand(nodes[i+1],1));\n",
    "                                            #create bias vectors (as columns)\n",
    "        end;\n",
    "        \n",
    "        return this;\n",
    "    end;\n",
    "end;\n",
    "\n",
    "function Activate(x)                    #this function will be applied to the weighted sum of the inputs\n",
    "                                        # before the value is passed on to the next layer\n",
    "    \n",
    "    #return max(0,x);                    #Rectified Linear Units aka ramp function\n",
    "    return 1 ./ (1 + exp(-x));\n",
    "    \n",
    "end;\n",
    "\n",
    "function GradActivate(x)                #gradient of the activation function at x\n",
    "    \n",
    "    #return 1*(x.> 0);                    #returns a subgradient of ramp\n",
    "    f = Activate(x); return f.*(1-f);\n",
    "    \n",
    "end;\n",
    "\n",
    "function FeedForward(net, input; p = 0.5)\n",
    "    \n",
    "    x = input;\n",
    "\n",
    "    for i = 1:net.layers-1                                      #for every layer\n",
    "        \n",
    "        x = Activate(p*net.weights[i]*x .+ net.biases[i]);       #weighted sum and apply activation function\n",
    "        \n",
    "    end;\n",
    "    \n",
    "    x = Activate(net.weights[net.layers]*x .+ net.biases[net.layers])    #do not bump the output layer\n",
    "    \n",
    "    return x';\n",
    "\n",
    "end;\n",
    "\n",
    "function BackPropogate(net, input, label; p = 0.5)\n",
    "\n",
    "    alpha = 0.1/net.layers;\n",
    "    alpha_bias = alpha/10;\n",
    "    \n",
    "    #FEED FORWARD FIRST\n",
    "    \n",
    "    x = Array[];                           #x will contain the pre-activated values\n",
    "    \n",
    "    y = Array[input];                      #y will contain the activated values\n",
    "\n",
    "    for i = 1:net.layers-1                   #for every layer\n",
    "        \n",
    "        dropout = (rand(size(net.weights[i])[1]) .< p);\n",
    "        \n",
    "        push!(x, net.weights[i]*y[i] + net.biases[i]);     #weighted sum\n",
    "        \n",
    "        push!(y, dropout .* Activate(x[i]));        #activated value\n",
    "        \n",
    "    end;\n",
    "    \n",
    "    #don't dropout the outputs\n",
    "    \n",
    "    push!(x, net.weights[net.layers]*y[net.layers] + net.biases[net.layers]);     #weighted sum\n",
    "        \n",
    "    push!(y, Activate(x[net.layers]));        #activated value\n",
    "    \n",
    "    #NOW PERFORM GRADIENT DESCENT\n",
    "    \n",
    "    error = vec((label - y[net.layers+1]) .* GradActivate(x[net.layers]));    #for gradient descent\n",
    "    \n",
    "    net.weights[net.layers] += alpha * error * y[net.layers]';                #adjust the weights at top level\n",
    "    \n",
    "    net.biases[net.layers] += alpha_bias * error;                                  #biases too\n",
    "    \n",
    "    for i = net.layers-1:-1:1                         #reverse order\n",
    "        \n",
    "        error = vec((net.weights[i+1]' * error));     #percolate remaining error to next level\n",
    "        \n",
    "        error = error .* GradActivate(x[i])           #error propogates to next layer\n",
    "        \n",
    "        net.weights[i] += alpha * error * y[i]';      #adjust the weights at this level\n",
    "        \n",
    "        net.biases[i] += alpha_bias * error;               #biases too\n",
    "        \n",
    "    end;\n",
    "    \n",
    "    #no return value -- all corrections are made in the net since it is passed by reference\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function Normalize(y)\n",
    "    y = y .- minimum(y,1);\n",
    "    y = y ./ (maximum(y,1));\n",
    "end;\n",
    "\n",
    "function Train(net, U_train, U_test, y_train, y_test; ITS = 1000, p = 0.5)\n",
    "    y_train = Normalize(y_train);\n",
    "    y_test = Normalize(y_test);\n",
    "    U_train = Normalize(U_train);\n",
    "    U_test = Normalize(U_test);\n",
    "\n",
    "    n = size(U_train)[1];\n",
    "\n",
    "    rd = rand(1:n, ITS, 1);\n",
    "\n",
    "    for i = 1:ITS\n",
    "        r = rd[i];\n",
    "        BackPropogate(net, U_train[r, :]', y_train[r]; p = p)\n",
    "    end;\n",
    "\n",
    "    #train_cost = 0.25*norm(FeedForward(net, U_train'; p = p) - y_train)^2;    #using half of the normal cost function since we are on [-1,1]\n",
    "    #test_cost = 0.25*norm(FeedForward(net, U_test'; p = p) - y_test)^2;    #using half of the normal cost function since we are on [-1,1]\n",
    "    \n",
    "    train_correct = ones(1,n)*((FeedForward(net, U_train'; p = p) .> 0.5) .== 1*(y_train .> 0.5));\n",
    "    train_correct = 100*train_correct[1,1]/n;\n",
    "    test_correct = ones(1,size(U_test)[1])*((FeedForward(net, U_test'; p = p) .> 0.5) .== (y_test .> 0.5));\n",
    "    test_correct = 100*test_correct[1,1]/size(U_test)[1];\n",
    "\n",
    "    return (train_correct, test_correct);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net1 = NeuralNet([2 15 15 15 15 15 15 15 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Correct = 50.0\n"
     ]
    }
   ],
   "source": [
    "#XOR TRAINING -- there is a lot of symmetry in this problem so it is difficult to train\n",
    "\n",
    "y = [0.07; 1.0; 0.7; 0.2];        #labels -- perturbations are included to break symmetry\n",
    "U = [0 0; 0 1; 1 0; 1 1];          #training data\n",
    "\n",
    "(train_correct, _) = Train(net1, U, U, y, y; ITS = 1000, p = 1.0)\n",
    "\n",
    "println(\"% Correct = \", train_correct);\n",
    "\n",
    "#println(net1);\n",
    "\n",
    "sleep(0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net4 = NeuralNet([1 15 15 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SIN TRAINING\n",
    "\n",
    "U_train = (0:0.01:1.5)'';\n",
    "y_train = sin(U_train);\n",
    "\n",
    "U_test = (0.05:0.01:1.55)'';\n",
    "y_test = sin(U_test);\n",
    "\n",
    "n = size(U_train)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Cost: 0.05417054737202084\n",
      "Test Cost: 0.041664639761063146\n"
     ]
    }
   ],
   "source": [
    "ITS = 1000000;\n",
    "p = 1.0;\n",
    "\n",
    "rd = rand(1:n, ITS, 1);\n",
    "\n",
    "for i = 1:ITS\n",
    "    r = rd[i];\n",
    "    BackPropogate(net4, U_train[r, :]', y_train[r]; p = p)\n",
    "end;\n",
    "\n",
    "train_cost = 0.5*norm(FeedForward(net4, U_train'; p = p) - y_train)^2;\n",
    "test_cost = 0.5*norm(FeedForward(net4, U_test'; p = p) - y_test)^2;\n",
    "    \n",
    "println(\"Train Cost: \", train_cost);\n",
    "println(\"Test Cost: \", test_cost);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151x1 Array{Float64,2}:\n",
       "  0.0535829 \n",
       "  0.0479565 \n",
       "  0.0424967 \n",
       "  0.0372081 \n",
       "  0.032095  \n",
       "  0.0271619 \n",
       "  0.0224127 \n",
       "  0.0178515 \n",
       "  0.013482  \n",
       "  0.00930759\n",
       "  0.0053316 \n",
       "  0.00155696\n",
       " -0.00201369\n",
       "  â‹®         \n",
       " -0.0389893 \n",
       " -0.0391978 \n",
       " -0.0393374 \n",
       " -0.0394071 \n",
       " -0.0394057 \n",
       " -0.0393324 \n",
       " -0.039186  \n",
       " -0.0389657 \n",
       " -0.0386706 \n",
       " -0.0382997 \n",
       " -0.0378524 \n",
       " -0.0373277 "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeedForward(net4, U_test'; p = p) - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_correct, test_correct) = Train(net4, U_train, U_test, y_train, y_test; ITS = 1000, p = 1.0)\n",
    "\n",
    "println(\"% Correct (Train) = \", train_correct);\n",
    "println(\"% Correct (Test)  = \", test_correct);\n",
    "\n",
    "sleep(0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet([3 100 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#College Admission Training  -- this data is hard to classify with high purity\n",
    "#because there are only 3 features and lots of variance\n",
    "\n",
    "(raw_data,headings) = readdlm(\"binary.csv\",','; header = true)\n",
    "y = raw_data[:, 1]\n",
    "U = raw_data[:, 2:4]           #this data set has three features\n",
    "\n",
    "n = size(U)[1];\n",
    "m = div(n,10);\n",
    "\n",
    "U_train = U[1:n-m , :];\n",
    "U_test = U[n-m+1:n, :];\n",
    "\n",
    "y_train = y[1:n-m, :];\n",
    "y_test = y[n-m+1:n, :];\n",
    "\n",
    "(train_correct, test_correct) = Train(net2, U_train, U_test, y_train, y_test; ITS = 100000, p = 1.0)\n",
    "\n",
    "println(\"% Correct (Train) = \", train_correct);\n",
    "println(\"% Correct (Test)  = \", test_correct);\n",
    "\n",
    "sleep(0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MNIST\n",
    "\n",
    "(U_test, y_test) = testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net3 = NeuralNet([784 400 10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 60000;\n",
    "ITS = 100000;\n",
    "p = 1.0;\n",
    "\n",
    "indices = eye(10);\n",
    "\n",
    "for i = 1:ITS\n",
    "    r = rand(1:n);\n",
    "    j = Int(trainlabel(r)+1);\n",
    "    BackPropogate(net3, trainfeatures(r)/256, indices[:,j]; p = p);\n",
    "end;\n",
    "\n",
    "train_correct = 0;\n",
    "for i = 1:100\n",
    "    train_correct += indmax(FeedForward(net3, trainfeatures(i)/256; p = p)) == trainlabel(i)+1;\n",
    "end;\n",
    "train_correct = train_correct[1,1];\n",
    "\n",
    "test_correct = 0;\n",
    "for i = 1:100\n",
    "    test_correct += indmax(FeedForward(net3, testfeatures(i)/256; p = p)) == testlabel(i)+1;\n",
    "end;\n",
    "\n",
    "test_correct = test_correct[1,1];\n",
    "\n",
    "println(\"% Correct (Train) = \", train_correct);\n",
    "println(\"% Correct (Test)  = \", test_correct);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
