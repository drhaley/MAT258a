{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtmin_test_wolfe(f,g,new_f,new_g,alpha,p)\n",
    "    c1 = 1e-4                #Armijo condition constant\n",
    "    c2 = 0.9                 #curvature condition constant\n",
    "    \n",
    "    armijo = new_f - f <= c1*alpha*(g'*p)[1,1]           #satisfies Armijo condition?\n",
    "    curvature = abs(new_g'*p)[1,1] > -c2*(g'*p)[1,1]     #satisfies curvature condition?\n",
    "    valley = (new_g'*p)[1,1] < 0                         #avoid hills?\n",
    "    return (armijo, curvature, valley)\n",
    "end\n",
    "\n",
    "function newtmin( obj, x0; maxIts=100, optTol=1e-6, BFGS=false)\n",
    "# Minimize a function f using Newtonâ€™s method.\n",
    "# obj: a function that evaluates the objective value, gradient, and Hessian at a point x, i.e.,\n",
    "#    (f, g, H) = obj(x)\n",
    "# x0: starting point.\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on ||grad(x)|| <= optTol*||grad(x0)||\n",
    "# BFGS (optional): set as true to use BFGS algorithm ( = false by default)\n",
    "\n",
    "    verbose = false\n",
    "    \n",
    "    its = 0;\n",
    "    x = x0;\n",
    "    \n",
    "    if(BFGS)\n",
    "        (f,g,_) = obj(x)\n",
    "        n = size(g)[1]\n",
    "        H_inv = eye(n)/norm(g)        #approximate Hessian as scaled identity\n",
    "    else\n",
    "        (f,g,H) = obj(x)     #evaluate gradient, et al\n",
    "    end\n",
    "      \n",
    "    ngx0 = norm(g)           #need to keep this value for stopping condition\n",
    "    \n",
    "    if(ngx0 > optTol^2)      #trap for low grad(x0)\n",
    "        \n",
    "        while(its < maxIts && norm(g) > optTol*(ngx0))\n",
    "            \n",
    "            Hessian_modded = false\n",
    "            \n",
    "            if(BFGS)\n",
    "                p = -H_inv * g\n",
    "            else\n",
    "                p = -H \\ g           #compute descent direction\n",
    "            end\n",
    "            \n",
    "            alpha = 1\n",
    "            alpha_min = 0\n",
    "            alpha_max = 1\n",
    "            \n",
    "            new_x = x + alpha*p  #trial step\n",
    "            \n",
    "            \n",
    "            (new_f,new_g,_) = obj(new_x)\n",
    "            \n",
    "            (armijo, curvature, valley) = newtmin_test_wolfe(f,g,new_f,new_g,alpha,p)\n",
    "                \n",
    "            while(!armijo || !valley)\n",
    "                \n",
    "                if(!Hessian_modded && !BFGS)\n",
    "                    #force the Hessian to be positive definite\n",
    "                    \n",
    "                    if(verbose) print(\"H\"); end\n",
    "                    \n",
    "                    Hessian_modded = true\n",
    "                \n",
    "                    (V, S) = eig(H)                    #decompose H\n",
    "                \n",
    "                    if(minimum(V) < 0)                 #if not positive definite\n",
    "                    \n",
    "                        #V[V .<= 0] = maximum(V)        #do not explore areas of negative curvature\n",
    "                        \n",
    "                        V = V + abs(1.01*minimum(V))\n",
    "                    \n",
    "                        V_inv = diagm(1 ./ V)\n",
    "                    \n",
    "                        p = -S*V_inv*S'*g              #recalculate p    \n",
    "                        \n",
    "                    end\n",
    "                \n",
    "                else    #Hessian was already fixed\n",
    "                \n",
    "                    if(!armijo)\n",
    "                        alpha_max = alpha                 #not enough decrease; try a smaller step\n",
    "                        if(verbose) print(\"A\"); end\n",
    "                    elseif(!curvature)\n",
    "                        if(verbose) print(\"C\"); end\n",
    "                        break                             #decrease is too slow to worry about hills/valleys\n",
    "                    else\n",
    "                        if(verbose) print(\"V\"); end\n",
    "                        alpha_min = alpha                 #going to land on a hill, try to speed over it\n",
    "                    end\n",
    "                                \n",
    "                    alpha = 0.5*(alpha_max + alpha_min)   #try an intermediate value\n",
    "                end\n",
    "                \n",
    "                new_x = x + alpha*p                   #trial step\n",
    "            \n",
    "                (new_f,new_g,_) = obj(new_x)      #evaluate new gradient, et al\n",
    "                \n",
    "                \n",
    "                (armijo, curvature, valley) = newtmin_test_wolfe(f,g,new_f,new_g,alpha,p)\n",
    "            \n",
    "                if (alpha_max - alpha_min < 0.01)       #interval is too narrow\n",
    "                    if(verbose) print(\"B\"); end\n",
    "                    break        #break out of loop; just take a step and hope for the best\n",
    "                end\n",
    "            \n",
    "            end        #terminate linesearch\n",
    "            \n",
    "            \n",
    "            #calculate Hessian\n",
    "            if (BFGS)\n",
    "                y = new_g - g\n",
    "                s = alpha*p\n",
    "                ro = 1 / (y'*s)[1,1]\n",
    "                H_inv = (eye(n) - ro*s*y')*H_inv*(eye(n) - ro*y*s') + ro*s*s'    \n",
    "            else\n",
    "                (_,_,H) = obj(new_x)\n",
    "            end\n",
    "            \n",
    "            x = new_x           #commit to the step\n",
    "            f = new_f\n",
    "            g = new_g\n",
    "            \n",
    "            Hessian_modded = false\n",
    "            \n",
    "            its = its + 1\n",
    "            \n",
    "        end\n",
    "    \n",
    "    else\n",
    "        println(\"grad(x0) is already < \", optTol^2)\n",
    "    end\n",
    "    \n",
    "    return (x, its)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 1.2082776971836815e-7 in 23 iterations\n"
     ]
    }
   ],
   "source": [
    "#test with a simple example function\n",
    "function fun(x)\n",
    "    f = x[1]^6 + x[2]^4 - x[1]*x[2]^3\n",
    "    g = [6x[1]^5 - x[2]^3; 4x[2]^3 - 3x[1]*x[2]^2]\n",
    "    H = [30x[1]^4     3x[2]^2              ;\n",
    "         3x[2]^2      12x[2]^2 - 6x[1]*x[2]]\n",
    "    return (f,g,H)\n",
    "end\n",
    "\n",
    "x0 = [-1;1]\n",
    "\n",
    "(f0,g0,H0) = fun(x0)\n",
    "\n",
    "(x, its) = newtmin( fun, x0;  BFGS = true)\n",
    "\n",
    "(f,g,H) = fun(x)\n",
    "\n",
    "println(\"Ratio: \", norm(g)/norm(g0), \" in \", its, \" iterations\")    #should be below optTol\n",
    "\n",
    "sleep(0.1)    #to keep output from bleeding into the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Toms566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem #1 solved in 30 iterations.  f* = 8.108845013103398e-17\n",
      "Problem #2 solved in 43 iterations.  f* = 0.005655649925499936\n",
      "Problem #3 solved in 11 iterations.  f* = 1.1279327696188855e-8\n",
      "Problem #4 solved in 168 iterations.  f* = 1.446993403066809e-20\n",
      "Problem #5 solved in 78 iterations.  f* = 3.622339039432224e-21\n",
      "Problem #6 solved in 27 iterations.  f* = 8.923832630175931e-5\n",
      "Problem #7 solved in 105 iterations.  f* = 1.3997601380989349e-6\n",
      "Problem #8 solved in 26 iterations.  f* = 0.0005415748924911815\n",
      "Problem #9 solved in 609 iterations.  f* = 88.03185764468085\n",
      "Problem #10 solved in 30 iterations.  f* = 1.792730780542459e-26\n",
      "Problem #11 solved in 41 iterations.  f* = 85822.2016263563\n",
      "Problem #12 solved in 54 iterations.  f* = 3.823631870661858e-20\n",
      "Problem #13 solved in 72 iterations.  f* = 2.139604678598898e-6\n",
      "Problem #14 solved in 42 iterations.  f* = 1.6645013453885573e-21\n",
      "Problem #15 solved in 78 iterations.  f* = 2.2111222450722513e-15\n",
      "Problem #16 solved in 17 iterations.  f* = 2.2159051551699382e-24\n",
      "Problem #17 solved in 38 iterations.  f* = 6.390171851248631e-17\n",
      "Problem #18 solved in 229 iterations.  f* = 0.005386315317011747\n"
     ]
    }
   ],
   "source": [
    "#test the algorithm on Toms566\n",
    "\n",
    "max_its = 2000\n",
    "tol = 1e-10\n",
    "\n",
    "for number = 1:18\n",
    "       \n",
    "    p = Problem(number)\n",
    "\n",
    "    (x, its) = newtmin(xk -> (p.obj(xk), p.grd(xk), p.hes(xk)), p.x0; maxIts = max_its, optTol = tol, BFGS = true)\n",
    "                                    #newtmin(anonymous function, p.x0)\n",
    "    \n",
    "    g = p.grd(x)\n",
    "    g0 = p.grd(p.x0)\n",
    "    f = p.obj(x)\n",
    "    f0 = p.obj(p.x0)\n",
    "    \n",
    "    if(norm(g)/norm(g0) <= tol || norm(g0) <= tol^2)\n",
    "        println(\"Problem #\", number, \" solved in \", its, \" iterations.  f* = \", f)\n",
    "        #println(\"f0: \", f0, \", f: \", f)\n",
    "    else\n",
    "        println(\"FAIL on Problem #\", number)#\": normg0:\", norm(g0), \", normg:\", norm(g))\n",
    "        #println(\"f0: \", f0, \", f: \", f)\n",
    "    end\n",
    "end\n",
    "\n",
    "sleep(0.1)    #to keep output from bleeding into the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalLogReg (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test on logistic regression set\n",
    "(raw_data,headings) = readdlm(\"binary.csv\",','; header = true)\n",
    "y = raw_data[:, 1]\n",
    "U = raw_data[:, 2:3]          #only 2:3 since instructions state only to use two features\n",
    "(m,n) = size(U)\n",
    "U = [U ones(m,1)]             #append 1s as the last feature (for regularization\n",
    "\n",
    "function evalLogReg(U,y, a)\n",
    "    (row,) = size(a)\n",
    "    x = exp(U*a)              #this is the \"inner\" function of the sigmoid\n",
    "    sig = x ./ (1 + x)        #value of the sigmoid for this iteration\n",
    "    \n",
    "    f = sum(-y .* U*a + log(1+x))[1,1]\n",
    "    g = U'*(sig - y)          #gradient of L\n",
    "    \n",
    "    gsig = diagm(sig.*(1-sig))    #sigmoid derivative without chain rule term, as a diagonal matrix\n",
    "    H = U'*gsig*U\n",
    "    \n",
    "    return (f,g,H)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 3.6033793994034693e-11 in 4 iterations\n"
     ]
    }
   ],
   "source": [
    "max_its = 100\n",
    "tol = 1e-6\n",
    "x0 = rand(3)/800\n",
    "(x, its) = newtmin(xk -> evalLogReg(U,y,xk), x0; maxIts = max_its, optTol = tol, BFGS = false)\n",
    "\n",
    "(f0,g0,H0) = evalLogReg(U,y,x0)\n",
    "(f,g,H) = evalLogReg(U,y,x)\n",
    "\n",
    "println(\"Ratio: \", norm(g)/norm(g0), \" in \", its, \" iterations\")    #should be below optTol\n",
    "\n",
    "\n",
    "#this took approximately 9,000 iterations using LLS (see HW2-alt.jl)\n",
    "#Now it only takes 3-4 iterations with this implementation of Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
