{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BFGSmin (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I have recoded the newton and BFGS code in order to more explicitly take advantage of linesearch\n",
    "# and to separate them from each other\n",
    "\n",
    "function sup_norm(x)                    #easier to read later code with this defined\n",
    "    return maximum(abs(x))\n",
    "end\n",
    "\n",
    "function test_armijo(f,g,new_f,alpha,p)\n",
    "    c = 1e-4\n",
    "    \n",
    "    return new_f - f <= c*alpha*(g'*p)[1,1]        #satisfies Armijo condition?\n",
    "end\n",
    "\n",
    "\n",
    "function newtmin( obj, x0; maxIts=100, optTol=1e-6)\n",
    "# Minimize a function f using Newtonâ€™s method.\n",
    "# obj: a function that evaluates the objective value, gradient, and Hessian at a point x, i.e.,\n",
    "#    (f, g, H) = obj(x)\n",
    "# x0: starting point.\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on ||grad(x)|| <= optTol*||grad(x0)||\n",
    "\n",
    "    its = 0;\n",
    "    x = x0;\n",
    "    \n",
    "    (f,g,H) = obj(x)     #evaluate gradient, et al\n",
    "      \n",
    "    ngx0 = sup_norm(g)           #need to keep this value for stopping condition\n",
    "    \n",
    "    if(ngx0 > optTol^2)      #trap for low grad(x0)\n",
    "        \n",
    "        while(its < maxIts && sup_norm(g) > optTol*(ngx0))\n",
    "            \n",
    "            Hessian_modded = false\n",
    "            \n",
    "            p = -H \\ g           #compute descent direction\n",
    "            \n",
    "            alpha = 1\n",
    "            alpha_min = 0\n",
    "            alpha_max = 1\n",
    "            \n",
    "            new_x = x + alpha*p  #trial step\n",
    "            \n",
    "            \n",
    "            (new_f,new_g,_) = obj(new_x)\n",
    "            \n",
    "            armijo = test_armijo(f,g,new_f,alpha,p)\n",
    "                \n",
    "            while(!armijo)\n",
    "                \n",
    "                if(!Hessian_modded)\n",
    "                    #force the Hessian to be positive definite\n",
    "                    \n",
    "                    Hessian_modded = true\n",
    "                \n",
    "                    (V, S) = eig(H)                    #decompose H\n",
    "                \n",
    "                    if(minimum(V) < 0)                 #if not positive definite\n",
    "                    \n",
    "                        #V[V .<= 0] = maximum(V)        #do not explore areas of negative curvature\n",
    "                        \n",
    "                        V = V + abs(1.01*minimum(V))\n",
    "                    \n",
    "                        V_inv = diagm(1 ./ V)\n",
    "                    \n",
    "                        p = -S*V_inv*S'*g              #recalculate p    \n",
    "                        \n",
    "                    end\n",
    "                \n",
    "                else    #Hessian was already fixed\n",
    "             \n",
    "                    alpha = alpha/2                #not enough decrease; try a smaller step\n",
    "    \n",
    "                end\n",
    "                \n",
    "                new_x = x + alpha*p                   #trial step\n",
    "            \n",
    "                (new_f,new_g,_) = obj(new_x)      #evaluate new gradient, et al\n",
    "                \n",
    "                \n",
    "                armijo = test_armijo(f,g,new_f,alpha,p)\n",
    "            \n",
    "                if (alpha < 0.01)       #interval is too narrow\n",
    "                    break        #break out of loop; just take a step and hope for the best\n",
    "                end\n",
    "            \n",
    "            end        #terminate linesearch\n",
    "            \n",
    "            (_,_,H) = obj(new_x)\n",
    "            \n",
    "            x = new_x           #commit to the step\n",
    "            f = new_f\n",
    "            g = new_g\n",
    "            \n",
    "            Hessian_modded = false\n",
    "            \n",
    "            its = its + 1\n",
    "            \n",
    "        end\n",
    "    \n",
    "    else\n",
    "        println(\"||grad(x0)|| is already < \", optTol^2)\n",
    "    end\n",
    "    \n",
    "    return (x, its)\n",
    "    \n",
    "end\n",
    "\n",
    "function test_wolfe(f,g,new_f,new_g,alpha, p)\n",
    "    c1 = 1e-4\n",
    "    c2 = 0.9                 #curvature condition constant\n",
    "    \n",
    "    armijo = new_f - f <= c1*alpha*(g'*p)[1,1]            #satisfies Armijo condition?\n",
    "    curvature = abs(new_g'*p)[1,1] <= -c2*(g'*p)[1,1]     #satisfies curvature condition?\n",
    "    downhill = (new_g'*p)[1,1] < 0\n",
    "    \n",
    "    return (armijo,curvature, downhill)\n",
    "end\n",
    "\n",
    "function BFGSmin( obj, x0; maxIts=100, optTol=1e-6)\n",
    "# Minimize a function f using BFGS.\n",
    "# obj: a function that evaluates the objective value and gradient at a point x, i.e.,\n",
    "#    (f, g) = obj(x)\n",
    "# x0: starting point.\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on ||grad(x)|| <= optTol*||grad(x0)||\n",
    "\n",
    "    \n",
    "    its = 0;\n",
    "    x = x0;\n",
    "    \n",
    "    (f,g) = obj(x)\n",
    "    n = size(g)[1]\n",
    "    H_inv = eye(n)/norm(g)        #approximate Hessian as scaled identity\n",
    "      \n",
    "    ngx0 = sup_norm(g)           #need to keep this value for stopping condition\n",
    "    \n",
    "    if(ngx0 > optTol^2)      #trap for low grad(x0)\n",
    "        \n",
    "        while(its < maxIts && sup_norm(g) > optTol*(ngx0))\n",
    "            \n",
    "            p = -H_inv * g           #compute descent direction\n",
    "                        \n",
    "            alpha = 1\n",
    "            alpha_min = 0\n",
    "            alpha_max = 1\n",
    "            \n",
    "            new_x = x + alpha*p  #trial step\n",
    "            \n",
    "            \n",
    "            (new_f,new_g) = obj(new_x)\n",
    "            \n",
    "            (armijo, curvature, downhill) = test_wolfe(f,g,new_f,new_g,alpha,p)\n",
    "                \n",
    "            while(!armijo || !curvature)\n",
    "                \n",
    "                if (!armijo || !downhill)\n",
    "                    alpha_max = alpha                 #slow down, not enough decrease/uphill\n",
    "                else\n",
    "                    alpha_min = alpha                 #on a downhill, need to speed up\n",
    "                end\n",
    "                             \n",
    "                alpha = 0.5*(alpha_max + alpha_min)   #try an intermediate value\n",
    "                \n",
    "                new_x = x + alpha*p                   #trial step\n",
    "            \n",
    "                (new_f,new_g) = obj(new_x)      #evaluate new gradient, et al\n",
    "                \n",
    "                \n",
    "                (armijo, curvature, downhill) = test_wolfe(f,g,new_f,new_g,alpha,p)\n",
    "            \n",
    "                if (alpha_max - alpha_min < 0.01)       #interval is too narrow\n",
    "                    break        #break out of loop; just take a step and hope for the best\n",
    "                end\n",
    "            \n",
    "            end        #terminate linesearch\n",
    "            \n",
    "            \n",
    "            #calculate Hessian\n",
    "            y = new_g - g\n",
    "            s = alpha*p\n",
    "            ro = 1 / (y'*s)[1,1]\n",
    "            H_inv = (eye(n) - ro*s*y')*H_inv*(eye(n) - ro*y*s') + ro*s*s'    \n",
    "            \n",
    "            x = new_x           #commit to the step\n",
    "            f = new_f\n",
    "            g = new_g\n",
    "            \n",
    "            its = its + 1\n",
    "            \n",
    "        end\n",
    "    \n",
    "    else\n",
    "        println(\"||grad(x0)|| is already < \", optTol^2)\n",
    "    end\n",
    "    \n",
    "    return (x, its)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AugLagrangeMin (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function AugLagrangeMin(f, g, H, C, Jt, x0;\n",
    "    eta = n -> 1/n, maxIts = 100, maxK=100, maxP = 1e10, optTol=1e-6, BFGS = false)\n",
    "        \n",
    "    k = 1\n",
    "    its = 0\n",
    "    p = 1e-4\n",
    "    x = x0\n",
    "    y = 0*C(x)\n",
    "    \n",
    "    while((sup_norm(C(x)) > optTol || sup_norm(g(x)-Jt(x)*y) > optTol) && k < maxK && p <= maxP)\n",
    "        \n",
    "        if(BFGS)\n",
    "            (x, new_its) = BFGSmin(x -> ( f(x) + p/2 * norm(C(x)-1/p*y)^2, g(x) + Jt(x)*(p*C(x)-y)), x;\n",
    "                maxIts = maxIts, optTol=optTol)\n",
    "        else\n",
    "            (x, new_its) = newtmin(x -> ( f(x) + p/2 * norm(C(x)-1/p*y)^2,\n",
    "                                          g(x) + Jt(x)*(p*C(x)-y),\n",
    "                                          H(x)+p*Jt(x)*Jt(x)'), x;\n",
    "                                         maxIts = maxIts, optTol=optTol)\n",
    "        end\n",
    "        \n",
    "        its = its + new_its\n",
    "        \n",
    "        if(sup_norm(C(x)) <= eta(k))\n",
    "            y = y - p*C(x)\n",
    "        else\n",
    "            p = 2*p\n",
    "        end\n",
    "        \n",
    "        k = k + 1\n",
    "    end\n",
    "\n",
    "    return (x, k, its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton solution: [1.0,1.0] -> 0.0 with 18 iterations\n",
      "BFGS solution: [1.0000000703774097,1.0001098935275672] -> -1.4075482424047436e-7 with 400 iterations\n"
     ]
    }
   ],
   "source": [
    "#Hock-Schittkowki 6\n",
    "#\n",
    "# min (1-x1)^2\n",
    "# s/t 10(x2-x1)^2 = 0\n",
    "#\n",
    "# start: (-1.2,1)\n",
    "# solution: (1,1) -> 0\n",
    "\n",
    "f = x -> 1-x[1,1]^2\n",
    "g = x -> [2*x[1,1]-2; 0]\n",
    "H = x -> [2 0; 0 0]\n",
    "C = x -> 10*(x[2,1]-x[1,1])^2\n",
    "Jt = x -> [-20*(x[2,1]-x[1,1]); 20*(x[2,1]-x[1,1])]\n",
    "x0 = [-1.2, 0]\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0)\n",
    "\n",
    "println(\"Newton solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0, BFGS = true)\n",
    "\n",
    "println(\"BFGS solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "#here Newton vastly outperforms the BFGS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton solution: [4316.245751202957,8.06905734337049e8] -> -8.069057175967665e8 with 4700 iterations\n",
      "BFGS solution: [-3.4597465383013144e-16,1.7320508124038025] -> -1.7320508124038025 with 127 iterations\n"
     ]
    }
   ],
   "source": [
    "#Hock-Schittkowki 7\n",
    "#\n",
    "# min ln(1+x1^2)-x2\n",
    "# s/t (1 + x1^2)^2 + x2^2 - 4 = 0\n",
    "#\n",
    "# start: (2,2)\n",
    "# solution: (0,1.73205) -> -1.73205\n",
    "\n",
    "f = x -> log(1+x[1,1]^2)-x[2,1]\n",
    "g = x -> [2*x[1,1]/(1+x[1,1]^2); -1]\n",
    "H = x -> [(2-2*x[1,1]^2)/(1+x[1,1]^2)^2 0; 0 0]\n",
    "C = x -> (1+x[1,1]^2)^2+x[2,1]^2-4\n",
    "Jt = x -> [4x[1,1]*(1+x[1,1]^2),2*x[2,1]]\n",
    "x0 = [2, 2]\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0)\n",
    "\n",
    "println(\"Newton solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0, BFGS = true)\n",
    "\n",
    "println(\"BFGS solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "#Here Newton is not able to produce a satisfactory answer.  In fact, due to the logarithm,\n",
    "#you can see from the Hessian that as x1 -> 1, the Hessian is no longer positive semidefinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton solution: [4.601594918576383,1.9558436075096002] -> -1 with 4 iterations\n",
      "BFGS solution: [4.601594811360296,1.9558437338668564] -> -1 with 212 iterations\n"
     ]
    }
   ],
   "source": [
    "#Hock-Schittkowki 8\n",
    "#\n",
    "# min -1\n",
    "# s/t x1^2+x2^2 - 25 = 0\n",
    "#     x1x2 - 9 = 0\n",
    "#\n",
    "# start: (2,1)\n",
    "# solution: x = (4.60159, 1.95585), -x, and their symmetric reflections -> -1\n",
    "\n",
    "f = x -> -1\n",
    "g = x -> [0;0]\n",
    "H = x -> [0 0;0 0]\n",
    "C = x -> [x[1,1]^2+x[2,1]^2 - 25; x[1,1]*x[2,1]-9]\n",
    "Jt = x -> [2x[1,1] x[2,1]; 2x[2,1] x[1,1]]\n",
    "x0 = [2, 1]\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0)\n",
    "\n",
    "println(\"Newton solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0, BFGS = true)\n",
    "\n",
    "println(\"BFGS solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "#Here again Newton vastly outperforms the slower BFGS algorithm for this problem with quadratic constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton solution: [1.3582536483894607e18,1.811004864519281e18] -> -0.29094937310538416 with 9900 iterations\n",
      "BFGS solution: [-3.0000001405311156,-4.000000141829025] -> -0.500000004471447 with 100 iterations\n"
     ]
    }
   ],
   "source": [
    "#Hock-Schittkowki 9\n",
    "#\n",
    "# min sin(pi*x1/12)cos(pi*x2/16)\n",
    "# s/t 4x1-3x2=0\n",
    "#\n",
    "# start: (0,0)\n",
    "# solution: (12k-3,16k-4) -> -1/2, k in Z\n",
    "\n",
    "f = x -> sin(pi*x[1,1]/12)*cos(pi*x[2,1]/16)\n",
    "g = x -> [pi/12*cos(pi*x[1,1]/12)*cos(pi*x[2,1]/16); -pi/16*sin(pi*x[1,1]/12)*sin(pi*x[2,1]/16)]\n",
    "H = x -> [-pi^2/12^2*sin(pi*x[1,1]/12)*cos(pi*x[2,1]/16) -pi^2/(12*16)*cos(pi*x[1,1]/12)*sin(pi*x[2,1]/16);\n",
    "          -pi^2/(12*16)*cos(pi*x[1,1]/12)*sin(pi*x[2,1]/16) -pi^2/16^2*sin(pi*x[1,1]/12)*cos(pi*x[2,1]/16)]\n",
    "C = x -> 4x[1,1]-3x[2,1]\n",
    "Jt = x -> [4; -3]\n",
    "x0 = [0, 0]\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0)\n",
    "\n",
    "println(\"Newton solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "(x, k, its) = AugLagrangeMin(f,g,H,C,Jt,x0, BFGS = true)\n",
    "\n",
    "println(\"BFGS solution: \", x, \" -> \", f(x), \" with \", its, \" iterations\")\n",
    "\n",
    "#For this very nonconvex problem, Newton again has difficulty, and does not even converge\n",
    "#but the BFGS algorithm efficiently arrives at the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
